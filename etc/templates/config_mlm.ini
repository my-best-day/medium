task-type = mlm
seq-len = 128
batch-size = 100
val-interval = 100
d-model = 768
heads = 12
n-layer = 6
dropout = 0.35

async-to-device: true
fused-adamw: true
compile: true

learning-rate = 1.0e-3
min-learning-rate = 5e-5
warmup-iters = 1000
lr-decay-iters = 45_000
max-iters = 60_000
weight-decay = 0.01

case = dickens # TODO: rename
base-dir = wiki
# supports round-robin of datasets
dataset-pattern = train_weke_128_*.msgpack
val-dataset-pattern = val_weke_128_*.msgpack
# DDP
dist-master-addr = 127.0.0.1
dist-master-port = 12233
# misc
max-checkpoints = 1
wandb = false
