task-type = gpt
seq-len = 128
batch-size = 100
val-interval = 100
d-model = 768
heads = 12
n-layer = 6
dropout = 0.1

async-to-device
fused-adamw
compile

learning-rate = 1e-3
min-learning-rate = 5e-5
warmup-iters = 1000
lr-decay-iters = 25_000
max-iters = 50_000
weight-decay = 0.01

case = dickens # change to something meaningful
base-dir = wiki
dataset-pattern = train_gpt.wiki_*.msgpack
val-dataset-pattern = val_gpt.wiki_*.msgpack

# DDP
dist-master-addr = 127.0.0.1
dist-master-port = 12233
# misc
max-checkpoints = 1
# wandb
