task-type = cola
seq-len = 128
batch-size = 64
val-interval = 200
d-model = 768
heads = 12
n-layer = 6
dropout = 0.0

async-to-device: true
fused-adamw: true
compile: true

learning-rate = 6e-5
min-learning-rate = 1e-7
warmup-iters = 300
lr-decay-iters = 8000
max-iters = 10000
weight-decay = 0.02

case = dickens # change to something meaningful
base-dir = wiki
# for sst2, the file names are hardcoded in sst2_dataset.py
# dataset-pattern = train_weke_256_*.msgpack
# val-dataset-pattern = val_weke_256_*.msgpack
# DDP
dist-master-addr = 127.0.0.1
dist-master-port = 12233
# misc
max-checkpoints = 1
wandb = false
