# Road blocks

* ~~Fine tune the model. So far I only pre-trained it~~ **Done!**
* Adjust the model to also support the GPT model and train it - still WIP, but pre-training works now.
* add fine-tuning of GPT
* ADD UNIT TESTS
* Create a inference use case example
* test performance using test splits
* Consider adopting more changes from DistilBERT


# Next tasks
* adjust readme to the new structure. add stuff about gpt, including results
* unit test ***
* see how to have a lr schedule for continuation from a checkpoint so that
  the lr starts with the value taken from cp, then "warms up" towards the
  target starting point, then continues per the configured lr schedule




-----
+ fix resuming from a checkpoint in torch_main.py
  . load the entire model state if checkpoint.task_type == config.task_type
  . load iteration, and other stuff not only if mlm but as long as the type is the same

? after the above is fixed, continue with gpt training using next_config_gpt.ini which
  adjust warmup iter, and decay-iter to have another bump in the learning rate

+ add initialization of the parameters / weights. Andrej: "important for GPT"
+ if not loading model state from checkpoint, use the above initialization, that is:
  + init_weights():
  + - init base model weights
  + - init lm model weights

-----
+ add cola
+ add GPT
+ move common code to common
+ add support for a test split / testing mode

+ for train, val, and test loss - sync up (across ranks) the total loss and total number of batches
  and the average loss is the ratio of the synced-up/accumulated loss / synced-up accumulated count

+ ditto for accuracy

+ ** we do do that * for MLM accuracy, we could have counted the total global number of masked tokens and the global
  total count of accurate tokens ... again, for now, I'll file it under not interesting

+ * comment added * instead of counting batches, count samples - items inside the batch and use that as a weight for
  the loss of each batch. actually, we are not going to do so, we might put a small comment saying
  we don't do that because the vast majority of our batches are complete and the number that are not
  is negligible

+ test continuing from a checkpoint with/out switch-training
+ test with CUDA
+ test with DDP


+ Need to complete mlm_task_handler, missing:
+   - method that drop predictions and original
+   - method that calculates val loss
+ also added methods to initialize the model and lm_head weights

+ Feed TaskHandler into TorchConfigurator - the handler is created
  inside the configurator

+ Use task_handler in TorchConfigurator



TaskHelper
  get_lm_model
      cola, sst2 -> BertClassifierModel
      mlm -> BertMlmModel
      gpt -> GptModel

  resume_from_checkpoint
      load the base-model
      if checkpoint.task_type == self.task_type
          load the lm_model
      else
          init weights lm_model

  debug_dump -
      for mlm logs the text with the predictions, compare to original tokens
      for gpt logs the prompt and the generated text

  estimate_accuracy
      gpt: skip (-1)
      mlm: (predicted == orig).count() / masked.count()
      cola, sst2: (predicted == label) / label.size()

  get_dataset
      ...
