# Road blocks

* ~~Fine tune the model. So far I only pre-trained it~~ **Done!**
* Adjust the model to also support the GPT model and train it - still WIP, but pre-training works now.
* add fine-tuning of GPT
* ADD UNIT TESTS
* Create a inference use case example
* test performance using test splits
* Consider adopting more changes from DistilBERT


# Next tasks
* fix resuming from a checkpoint in torch_main.py
- load the entire model state if checkpoint.task_type == config.task_type
- load iteration, and other stuff not only if mlm but as long as the type is the same

* after the above is fixed, continue with gpt training using next_config_gpt.ini which
adjust warmup iter, and decay-iter to have another bump in the learning rate

* adjust readme to the new structure. add stuff about gpt, including results

* add initialization of the parameters / weights "important for GPT"
* if not loading model state from checkpoint, use the above initialization, that is:
- init_weights():
-- init base model weights
-- init lm model weights

** unit test **


NEXT:
- add cola
- add GPT
- move common code to common
- unit tests
- add support for a test split / testing mode





+ test continuing from a checkpoint with/out switch-training
+ test with CUDA
+ test with DDP


+ Need to complete mlm_task_handler, missing:
+   - method that drop predictions and original
+   - method that calculates val loss
+ also added methods to initialize the model and lm_head weights

+ Feed TaskHandler into TorchConfigurator - the handler is created
  inside the configurator

+ Use task_handler in TorchConfigurator



TaskHelper
  get_lm_model
      cola, sst2 -> BertClassifierModel
      mlm -> BertMlmModel
      gpt -> GptModel

  resume_from_checkpoint
      load the base-model
      if checkpoint.task_type == self.task_type
          load the lm_model
      else
          init weights lm_model

  debug_dump -
      for mlm logs the text with the predictions, compare to original tokens
      for gpt logs the prompt and the generated text

  estimate_accuracy
      gpt: skip (-1)
      mlm: (predicted == orig).count() / masked.count()
      cola, sst2: (predicted == label) / label.size()

  get_dataset
      ...
